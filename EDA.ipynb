{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Alexander Hamilton, John Jay, and John Adams \n",
    "\n",
    "The purpose of this script is to conduct basic exploratory data analysis on  the 85 essays in the Federalist Papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data and import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### NLTK Download\n",
    "# Note: To download nltk products, you need to run the nltk downloader. If you \n",
    "# just want to run this quickly, uncomment the following line and run:\n",
    "# nltk.download('popular')\n",
    "\n",
    "# Note that I created a Spyder prjoect in my federalist-papers-nlp folder so I\n",
    "# can just reference the \"Data\" folder without all the stuff that comes before it.\n",
    "fed_papers = pd.read_csv(\"Data/full_fedpapers.csv\")\n",
    "\n",
    "print(fed_papers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#                             Data Prep\n",
    "# ----------------------------------------------------------------------------\n",
    "# First, let's create a few dataframes that can be used for analysis purposes later on\n",
    "# Before we move on, there are a lot of unnecessary words here! Let's filter\n",
    "# some of these (stop words) out.\n",
    "stop_words = ['would', 'may', 'yet', 'must', 'shall', 'not', 'still', 'let', \n",
    "              'also', 'ought', 'a', 'the', 'it', 'i', 'upon', 'but', 'if', 'in',\n",
    "              'this', 'might', 'and', 'us', 'can', 'as', 'to']\n",
    "\n",
    "fed_nonstop = fed_papers.copy()\n",
    "fed_nonstop = fed_nonstop[~fed_nonstop['word'].isin(stop_words)]\n",
    "\n",
    "# It also looks like there are words that should be counted together (i.e. state\n",
    "# and states). Let's use a lemmatizer to solve this.\n",
    "\n",
    "\n",
    "\n",
    "# Start by creating a grouped dataframe of our word counts\n",
    "word_counts = fed_nonstop.groupby(['word']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name = 'count') \\\n",
    "    .sort_values('count', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "print(word_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#                             Viz 1: Top 20 Words\n",
    "# ----------------------------------------------------------------------------\n",
    "#%% Our first visualization counts the top 20 words across all documents.\n",
    "# Set the theme\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Build the visualization\n",
    "viz1 = sns.barplot(x = 'count',\n",
    "            y = 'word',\n",
    "            data = words_nonstop[:20],\n",
    "            palette = \"Purples_r\")\n",
    "\n",
    "# Set our labels\n",
    "viz1.set(xlabel='Number of Appearances', ylabel='Word', title = 'Word Counts across all Federalist Papers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#                          Viz 2: Document Lengths\n",
    "# ----------------------------------------------------------------------------\n",
    "#%% Our second visualization will look at the lengths of each document,\n",
    "# as well as the average length of each one.\n",
    "doc_lengths = fed_papers.groupby(['essay']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name = 'length') \\\n",
    "    .sort_values('length', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "viz2 = sns.violinplot(y = doc_lengths['length'], \n",
    "               color = \"Slateblue\")\n",
    "\n",
    "# Set our labels\n",
    "viz2.set(ylabel = 'Number of Words', title = 'Length of Federalist Papers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#                      Viz 3: Document Lengths by Author\n",
    "# ----------------------------------------------------------------------------\n",
    "#%% Our third visualization will look at the lengths of each document,\n",
    "# as well as the average length of each one, disaggregated by author\n",
    "doc_lengths = fed_papers.groupby(['essay', 'Author']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name = 'length') \\\n",
    "    .sort_values('length', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "viz3 = sns.catplot(x = 'Author',\n",
    "                      y = 'length',\n",
    "                      data = doc_lengths,\n",
    "                      hue = 'Author',\n",
    "                      palette = 'Purples_r')\n",
    "                      # color = \"Slateblue\")\n",
    "\n",
    "# Set our labels\n",
    "viz3.set(xlabel = 'Author', ylabel = 'Number of Words', title = 'Length of Federalist Papers by Author')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#                         Viz 4-7: Top 10 Words by Author\n",
    "# ----------------------------------------------------------------------------\n",
    "# Our fourth  through seventh  visualization constitutes a bar chart of the top 10 words  \n",
    "# by word count of each author (John Jay, Alexander Hamilton, James Madison, or \n",
    "# Unknown).\n",
    "\n",
    "#Hamilton - Visualization 4------------------------------------------------------\n",
    "\n",
    "doc_lengths = fed_papers.groupby(['Author','word']) \\\n",
    "    .Essay.count() \\\n",
    "    .reset_index(name = 'count') \\\n",
    "    .sort_values('count', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "Hamilton_words = doc_lengths.loc[doc_lengths.Author == 'Hamilton']\n",
    "Hamilton_top_words = Hamilton_words.head(17)\n",
    "\n",
    "\n",
    "Hamilton_top_words = Hamilton_top_words.copy()\n",
    "Hamilton_top_words = Hamilton_top_words[~Hamilton_top_words['word'].isin(stop_words)]\n",
    "\n",
    "# Set the theme\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Build the visualization\n",
    "viz4 = sns.barplot(x = 'word',\n",
    "            y = 'count',\n",
    "            data = Hamilton_top_words,\n",
    "            palette = \"flare\")\n",
    "\n",
    "#Rotate X tick labels\n",
    "viz4.set_xticklabels(viz5.get_xticklabels(), rotation=45 )\n",
    "\n",
    "# Set our labels\n",
    "viz4.set(xlabel='word', ylabel='count', title = 'Hamilton Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#John Jay - Visualization 5--------------------------------------------------\n",
    "\n",
    "doc_lengths = fed_papers.groupby(['Author','word']) \\\n",
    "    .Essay.count() \\\n",
    "    .reset_index(name = 'count') \\\n",
    "    .sort_values('count', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "Jay_top = doc_lengths.loc[doc_lengths.Author == 'Jay']\n",
    "Jay_top_words = Jay_top.head(17)\n",
    "\n",
    "\n",
    "\n",
    "Jay_top_words = Jay_top_words.copy()\n",
    "Jay_top_words = Jay_top_words[~Jay_top_words['word'].isin(stop_words)]\n",
    "\n",
    "# Set the theme\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Build the visualization\n",
    "viz5 = sns.barplot(x = 'word',\n",
    "            y = 'count',\n",
    "            data = Jay_top_words,\n",
    "            palette = \"mako\")\n",
    "\n",
    "#Rotate X tick labels\n",
    "viz5.set_xticklabels(viz6.get_xticklabels(), rotation=45 )\n",
    "\n",
    "# Set our labels\n",
    "viz5.set(xlabel='word', ylabel='count', title = 'Jay Top Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Madison - Visualization 6-------------------------------------------------------\n",
    "\n",
    "doc_lengths = fed_papers.groupby(['Author','word']) \\\n",
    "    .Essay.count() \\\n",
    "    .reset_index(name = 'count') \\\n",
    "    .sort_values('count', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "Madison_top = doc_lengths.loc[doc_lengths.Author == 'Madison']\n",
    "Madison_top_words = Madison_top.head(15)\n",
    "\n",
    "Madison_top_words = Madison_top_words.copy()\n",
    "Madison_top_words = Madison_top_words[~Madison_top_words['word'].isin(stop_words)]\n",
    "\n",
    "# Set the theme\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Build the visualization\n",
    "viz6 = sns.barplot(x = 'word',\n",
    "            y = 'count',\n",
    "            data = Madison_top_words,\n",
    "            palette = \"coolwarm\")\n",
    "\n",
    "#Rotate X tick labels\n",
    "viz6.set_xticklabels(viz7.get_xticklabels(), rotation=45 )\n",
    "\n",
    "# Set our labels\n",
    "viz6.set(xlabel='word', ylabel='count', title = 'Madison Top Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unknown - Visualization 7-------------------------------------------------------\n",
    "\n",
    "doc_lengths = fed_papers.groupby(['Author','word']) \\\n",
    "    .Essay.count() \\\n",
    "    .reset_index(name = 'count') \\\n",
    "    .sort_values('count', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "Unknown_top = doc_lengths.loc[doc_lengths.Author == 'Unknown']\n",
    "Unknown_top_words = Unknown_top.head(19)\n",
    "\n",
    "\n",
    "Unknown_top_words = Unknown_top_words.copy()\n",
    "Unknown_top_words = Unknown_top_words[~Unknown_top_words['word'].isin(stop_words)]\n",
    "\n",
    "# Set the theme\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Build the visualization\n",
    "viz7 = sns.barplot(x = 'word',\n",
    "            y = 'count',\n",
    "            data = Unknown_top_words,\n",
    "            palette = \"YlOrBr\")\n",
    "\n",
    "#Rotate X tick labels\n",
    "viz7.set_xticklabels(viz8.get_xticklabels(), rotation=45 )\n",
    "\n",
    "# Set our labels\n",
    "viz7.set(xlabel='word', ylabel='count', title = 'Unknown Top Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#                      Viz 8: Word Count vs. Word Frequency\n",
    "# ----------------------------------------------------------------------------\n",
    "#%% Our eighth visualization constitues a scatter plot of all the words\n",
    "# that could reasonably appear in our dataset, measuring the number of times\n",
    "# each one appears as well as the number of documents it appears in.\n",
    "\n",
    "# The hope here is to take a look at what will eventually be the TF-IDF of each\n",
    "# word: that way we can filter out words that appear many times but only in very\n",
    "# few documents (i.e. 'Constitution' appears 100 times in total but 95 times\n",
    "# in Essay 100.)\n",
    "\n",
    "# Now let's create a grouped dataframe that counts the number of documents\n",
    "# a given word appears in (document frequency). This is important to help us identify\n",
    "# words that may appear many times but in the same document. A word is considered\n",
    "# more \"important\" if it is not just a frequently occuring word within a document, but a word that\n",
    "# appears across many documents\n",
    "# doc_lengths = fed_nonstop.groupby(['word','Essay']) \\\n",
    "#     .Essay.count() \\\n",
    "#     .reset_index(name = 'count') \\\n",
    "#     .sort_values('count', ascending = False) \\\n",
    "#     .reset_index(drop = True)\n",
    "\n",
    "doc_lengths = fed_nonstop[['word', 'Essay']].drop_duplicates() \\\n",
    "    .groupby(['word']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name = 'doc_count') \\\n",
    "    .sort_values('doc_count', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "#Looking at which essays government and other words appears more frequently \n",
    "# word_frequency = doc_lengths.loc[doc_lengths.word.isin([ 'one', 'government', 'people'])]\n",
    "\n",
    "#If we wanted to look at the percentage in total papers\n",
    "##word_frequency['% of total papers'] = word_frequency['count'] / word_frequency['count'].sum()\n",
    "\n",
    "#Remove \"Essay\" from the Essay columns so we are only left with the number - just so we can fit everything into the graph\n",
    "# word_frequency['Essay'] = pd.to_numeric(word_frequency['Essay'].astype(str).str[5:], errors='coerce')\n",
    "\n",
    "# word_frequency.head()\n",
    "\n",
    "merged_counts = pd.merge(word_counts, \n",
    "                         doc_lengths, \n",
    "                         left_on = 'word', \n",
    "                         right_on = 'word',\n",
    "                         how = 'inner')\n",
    "\n",
    "# Resize the plot\n",
    "plt.figure(figsize=(10,5))\n",
    "viz8 = sns.scatterplot(data = merged_counts, \n",
    "                       x = \"doc_count\", \n",
    "                       y = \"count\",\n",
    "                       alpha = .3,\n",
    "                       color = \"slateblue\")\n",
    "\n",
    "# Set our labels\n",
    "viz8.set(ylabel = 'Word Frequency', \n",
    "         xlabel = 'Document Frequency',\n",
    "         title = 'Word Frequency by Document Frequency')\n",
    "\n",
    "#Redo the x axis ticks \n",
    "\n",
    "# viz8.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "# viz8.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#                                   TF-IDF\n",
    "# ----------------------------------------------------------------------------\n",
    "# Building on our analysis above, we'll now look into the TF-IDF for each word.\n",
    "####W should look for key words that would are unique to each author. Eventually do TF-IDF code here. \n",
    "\n",
    "# Let's start by calculating term frequency. While we've mostly been looking at the word counts\n",
    "# across all documents, for term frequency, we care about the propoortion of times\n",
    "# the word appears in a given document. Ex: If a sentence is 10 words long and \n",
    "# 'constitution' appears 3 times, its term frequency is .3 (30%).\n",
    "fed_analysis = merged_counts.copy()\n",
    "\n",
    "# Calculate the length of each essay\n",
    "doc_lengths = fed_nonstop.groupby(['Essay']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name = 'doc_length') \\\n",
    "    .reset_index(drop = True)\n",
    "    \n",
    "# Now let's figure out how many times a word appears in a given essay\n",
    "word_frequency = fed_nonstop.groupby(['word', 'Essay']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name = 'word_freq') \\\n",
    "    .sort_values('word') \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "# With these two dataframes, we can bring them together to calculate our tf score\n",
    "merged_tf = pd.merge(word_frequency, \n",
    "                     doc_lengths, \n",
    "                     left_on = 'Essay',\n",
    "                     right_on = 'Essay',\n",
    "                     how = 'inner')\n",
    "\n",
    "merged_tf['tf'] = merged_tf['word_freq'] / merged_tf['doc_length']\n",
    "\n",
    "\n",
    "# We can pull the inverse document frequency from our merged_counts dataframe above\n",
    "fed_analysis['idf'] = np.log(85 / fed_analysis['doc_count'])\n",
    "\n",
    "# Let's merge these (again) into one big dataframe\n",
    "tf_idf_df = pd.merge(merged_tf,\n",
    "                     fed_analysis,\n",
    "                     left_on = 'word',\n",
    "                     right_on = 'word',\n",
    "                     how = 'inner')\n",
    "\n",
    "tf_idf_df['tf_idf'] = tf_idf_df['tf'] * tf_idf_df['idf']\n",
    "\n",
    "\n",
    "#%%\n",
    "# ----------------------------------------------------------------------------\n",
    "#                                Viz 9: Top TF-IDF\n",
    "# ----------------------------------------------------------------------------\n",
    "# Let's see which words have the highest TF-IDF scores by author. This will\n",
    "# help us identify the style of each author by looking at the words that they\n",
    "# use most uniquely.\n",
    "\n",
    "authors = fed_nonstop[['essay', 'Author']].drop_duplicates()\n",
    "\n",
    "merged_df = tf_idf_df.merge(authors,\n",
    "                            left_on = 'Essay',\n",
    "                            right_on = 'essay')\n",
    "\n",
    "authors_tf = merged_df.groupby(['tf_idf', 'Author', 'word']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name = 'tfidf') \\\n",
    "    .sort_values('tf_idf', ascending = False) \\\n",
    "    .reset_index(drop = True)\n",
    "    \n",
    "# Find our top 10 words for each author\n",
    "authors_top_tf = authors_tf.groupby('Author')['tf_idf'] \\\n",
    "    .nlargest(10, keep = 'first') \\\n",
    "    .reset_index(name = \"tf_idf\")\n",
    "\n",
    "# Unfortunately this drops the actual word, so let's merge it back on\n",
    "authors_top_tf = authors_top_tf.merge(authors_tf,\n",
    "                                      left_on = ['Author', 'tf_idf'],\n",
    "                                      right_on = ['Author', 'tf_idf'])\n",
    "\n",
    "# Set the theme\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Build the visualization\n",
    "viz9 = sns.FacetGrid(authors_top_tf, \n",
    "                     col = \"Author\", \n",
    "                     sharex = False, \n",
    "                     sharey = False\n",
    "                     )\n",
    "viz9.map(sns.barplot, \"tf_idf\", \"word\")\n",
    "\n",
    "\n",
    "# Set our labels\n",
    "# Set our labels\n",
    "viz9.set(xlabel='tf_idf', ylabel='word', title = 'Hamilton Top Words')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
